#imports
!pip install fuzzywuzzy python-Levenshtein

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import re
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
from difflib import SequenceMatcher
```

#read data
data = pd.read_csv('Student Entrada Queries.csv', header=9, nrows=10000)
df = pd.DataFrame(data)

pd.set_option('display.max_rows', 10)

```

# rename columns to Queries and Event Count (renames first event count column bc student data)
df = df.rename(columns={'Page path + query string': 'Queries', 'Event count': 'Event Count'})

```
#drop NaN values in Queries column
#1533 rows without dropna, 1532 with
df.dropna(subset=['Queries'], inplace=True)
```

# make df only have 2 columns
df = df[['Queries', 'Event Count']]

# lowercasing result_sorted
df['Queries'] = df['Queries'].str.lower()

```

#remove rows with event count of 0
df = df[df['Event Count'] != 0]

#1061 rows
```

# clean  text from queries column
import re

#replace + with spaces
df['Queries'] = df['Queries'].str.replace('+', ' ')
df['Queries'] = df['Queries'].str.replace('%22', '')

# clean search results mostly
def extract_search_term(query):
  match = re.search(r"q=([^&]+)", query)  # Find 'q=' followed by characters until '&'
  if match:
    return match.group(1)  # Extract the captured search term
  else:
    return query  # Or handle cases with no match as needed

df['Queries'] = df['Queries'].apply(extract_search_term)


mask = df['Queries'].str.startswith('/curriculum')
df.loc[mask, 'Queries'] = 'filters only'
```

# sort by descending event count
df = df.sort_values(by=['Event Count'], ascending=False)

```

#1061 rows x 2 columns
pd.set_option('display.max_rows', 10)
print(df)

```

# manual mapping of similar terms that combine_similar won't pick up

manual_mapping = {
    'trauma informed':'trauma informed practice',
    'neonat': 'neonatology',
    'attention deficit':'attention deficit hyperactivity disorder',
    'trauma-inform':'trauma informed practice',
    'nortth':'north',
    'shadow':'shadowing'
}

df['Queries'] = df['Queries'].map(manual_mapping).fillna(df['Queries'])

```

#combines similar queries

def similarity_ratio(a, b):
    if pd.isna(a) or pd.isna(b):
        return 0
    return SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()
    # takes 2 strings, finds same letters/characters, length of characters that are the same
    # return similarity score (DOUBLE) based on sizes of similar chunks

def combine_similar(df, query_col='Queries', count_col='Event Count', threshold=0.9):
  # 0.9 = 744 rows
    combined = {}
    for i, row in df.iterrows():
        if pd.isna(row[query_col]):
            continue
        matched = False
        for key in combined:
            if similarity_ratio(row[query_col], key) >= threshold:
              #if similar, combine the two and make 1 row
                combined[key] += row[count_col]
                matched = True
                break
        if not matched:
            #if not matched make separate
            combined[row[query_col]] = row[count_col]
    return pd.DataFrame({query_col: combined.keys(), count_col: combined.values()})
    # go thru rows of queries and compares to each other

# Load your CSV file

result = combine_similar(df)
result_sorted = result.sort_values('Event Count', ascending=False).reset_index(drop=True)
pd.set_option('display.max_rows', 10)


# Display all rows
print(result_sorted)
#744 rows

```
