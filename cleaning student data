#imports
!pip install fuzzywuzzy python-Levenshtein

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import re
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
from difflib import SequenceMatcher
```

#read data
data = pd.read_csv('Student Entrada Queries.csv', header=9, nrows=10000)
df = pd.DataFrame(data)

pd.set_option('display.max_rows', 10)

```

# rename columns to Queries and Event Count (renames first event count column bc student data)
df = df.rename(columns={'Page path + query string': 'Queries', 'Event count': 'Event Count'})

```
#drop NaN values in Queries column
#1533 rows without dropna, 1532 with
df.dropna(subset=['Queries'], inplace=True)
```

# make df only have 2 columns
df = df[['Queries', 'Event Count']]

# lowercasing result_sorted
df['Queries'] = df['Queries'].str.lower()

```

#remove rows with event count of 0
df = df[df['Event Count'] != 0]

#1061 rows
```

# clean  text from queries column
import re

#replace + with spaces
df['Queries'] = df['Queries'].str.replace('+', ' ')
df['Queries'] = df['Queries'].str.replace('%22', '')

# clean search results mostly
def extract_search_term(query):
  match = re.search(r"q=([^&]+)", query)  # Find 'q=' followed by characters until '&'
  if match:
    return match.group(1)  # Extract the captured search term
  else:
    return query  # Or handle cases with no match as needed

df['Queries'] = df['Queries'].apply(extract_search_term)


mask = df['Queries'].str.startswith('/curriculum')
df.loc[mask, 'Queries'] = 'filters only'
```

# sort by descending event count
df = df.sort_values(by=['Event Count'], ascending=False)

```

#1061 rows x 2 columns
pd.set_option('display.max_rows', 10)
print(df)

```

# manual mapping of similar terms that combine_similar won't pick up

manual_mapping = {
    'trauma informed':'trauma informed practice',
    'neonat': 'neonatology',
    'attention deficit':'attention deficit hyperactivity disorder',
    'trauma-inform':'trauma informed practice',
    'nortth':'north',
    'shadow':'shadowing'
}

df['Queries'] = df['Queries'].map(manual_mapping).fillna(df['Queries'])

```

#combines similar queries

def similarity_ratio(a, b):
    if pd.isna(a) or pd.isna(b):
        return 0
    return SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()
    # takes 2 strings, finds same letters/characters, length of characters that are the same
    # return similarity score (DOUBLE) based on sizes of similar chunks

def combine_similar(df, query_col='Queries', count_col='Event Count', threshold=0.9):
  # 0.9 = 744 rows
    combined = {}
    for i, row in df.iterrows():
        if pd.isna(row[query_col]):
            continue
        matched = False
        for key in combined:
            if similarity_ratio(row[query_col], key) >= threshold:
              #if similar, combine the two and make 1 row
                combined[key] += row[count_col]
                matched = True
                break
        if not matched:
            #if not matched make separate
            combined[row[query_col]] = row[count_col]
    return pd.DataFrame({query_col: combined.keys(), count_col: combined.values()})
    # go thru rows of queries and compares to each other

# Load your CSV file

result = combine_similar(df)
result_sorted = result.sort_values('Event Count', ascending=False).reset_index(drop=True)
pd.set_option('display.max_rows', 10)


# Display all rows
print(result_sorted)
#735 rows

```
import matplotlib.pyplot as plt
import seaborn as sns

# Sort the DataFrame by 'Event Count' in descending order and take the top 20
top_20_df = result_sorted.sort_values(by=['Event Count'], ascending=False).head(10)

# Create the bar plot using the top 20 DataFrame
sns.barplot( x='Event Count', y='Queries', data=top_20_df, palette = 'rainbow') 


#plt.xticks(rotation=90) # Rotate x-axis labels for better readability (optional)

plt.show()

plt.savefig('student_data.png')

###################

#acronym dictionary, expands acronyms and applies them
#acronym dictionary
acronym_map = {
    'adhd': 'attention deficit hyperactivity disorder',
    'cbl': 'case based learning',
    'ot': 'occupational therapy',
    'carms': 'canadian resident matching service',
    'nicu': 'neonatal intensive care unit',
    'spf': 'summative portfolio form',
    #'npm': 'northern medical program',
    'qips': 'quality improvement and patient safety',
    'wba': 'workplace based assessment',
    'epa': 'entrustable professional activity',
    'osce': 'objective structured clinical examination'
}

# Function to expand acronyms
def expand_acronyms(text):
  #takes text, isolates it and replaces w longer version of word
    words = text.split()
    expanded_words = [acronym_map.get(word, word) for word in words]
    return ' '.join(expanded_words)


# Apply preprocessing
result_sorted['Queries'] = result_sorted['Queries'].apply(expand_acronyms)

pd.set_option('display.max_rows', 10)

print(result_sorted)

############################

search_value = ["policy", "policies", ] # Replace with the specific value you're looking for
result_row = result_sorted[result_sorted['Queries'].str.contains('|'.join(search_value), case=False, na=False)]

display(result_row)


